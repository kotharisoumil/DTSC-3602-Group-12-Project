{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Baseline Representation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DR_NO</th>\n",
       "      <th>Date Rptd</th>\n",
       "      <th>DATE OCC</th>\n",
       "      <th>TIME OCC</th>\n",
       "      <th>AREA</th>\n",
       "      <th>AREA NAME</th>\n",
       "      <th>Rpt Dist No</th>\n",
       "      <th>Part 1-2</th>\n",
       "      <th>Crm Cd</th>\n",
       "      <th>Crm Cd Desc</th>\n",
       "      <th>...</th>\n",
       "      <th>Status</th>\n",
       "      <th>Status Desc</th>\n",
       "      <th>Crm Cd 1</th>\n",
       "      <th>Crm Cd 2</th>\n",
       "      <th>Crm Cd 3</th>\n",
       "      <th>Crm Cd 4</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>Cross Street</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>211507896</td>\n",
       "      <td>04/11/2021 12:00:00 AM</td>\n",
       "      <td>11/07/2020 12:00:00 AM</td>\n",
       "      <td>845</td>\n",
       "      <td>15</td>\n",
       "      <td>N Hollywood</td>\n",
       "      <td>1502</td>\n",
       "      <td>2</td>\n",
       "      <td>354</td>\n",
       "      <td>THEFT OF IDENTITY</td>\n",
       "      <td>...</td>\n",
       "      <td>IC</td>\n",
       "      <td>Invest Cont</td>\n",
       "      <td>354.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7800    BEEMAN                       AV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.2124</td>\n",
       "      <td>-118.4092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201516622</td>\n",
       "      <td>10/21/2020 12:00:00 AM</td>\n",
       "      <td>10/18/2020 12:00:00 AM</td>\n",
       "      <td>1845</td>\n",
       "      <td>15</td>\n",
       "      <td>N Hollywood</td>\n",
       "      <td>1521</td>\n",
       "      <td>1</td>\n",
       "      <td>230</td>\n",
       "      <td>ASSAULT WITH DEADLY WEAPON, AGGRAVATED ASSAULT</td>\n",
       "      <td>...</td>\n",
       "      <td>IC</td>\n",
       "      <td>Invest Cont</td>\n",
       "      <td>230.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ATOLL                        AV</td>\n",
       "      <td>N  GAULT</td>\n",
       "      <td>34.1993</td>\n",
       "      <td>-118.4203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>240913563</td>\n",
       "      <td>12/10/2024 12:00:00 AM</td>\n",
       "      <td>10/30/2020 12:00:00 AM</td>\n",
       "      <td>1240</td>\n",
       "      <td>9</td>\n",
       "      <td>Van Nuys</td>\n",
       "      <td>933</td>\n",
       "      <td>2</td>\n",
       "      <td>354</td>\n",
       "      <td>THEFT OF IDENTITY</td>\n",
       "      <td>...</td>\n",
       "      <td>IC</td>\n",
       "      <td>Invest Cont</td>\n",
       "      <td>354.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14600    SYLVAN                       ST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.1847</td>\n",
       "      <td>-118.4509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>210704711</td>\n",
       "      <td>12/24/2020 12:00:00 AM</td>\n",
       "      <td>12/24/2020 12:00:00 AM</td>\n",
       "      <td>1310</td>\n",
       "      <td>7</td>\n",
       "      <td>Wilshire</td>\n",
       "      <td>782</td>\n",
       "      <td>1</td>\n",
       "      <td>331</td>\n",
       "      <td>THEFT FROM MOTOR VEHICLE - GRAND ($950.01 AND ...</td>\n",
       "      <td>...</td>\n",
       "      <td>IC</td>\n",
       "      <td>Invest Cont</td>\n",
       "      <td>331.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6000    COMEY                        AV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.0339</td>\n",
       "      <td>-118.3747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201418201</td>\n",
       "      <td>10/03/2020 12:00:00 AM</td>\n",
       "      <td>09/29/2020 12:00:00 AM</td>\n",
       "      <td>1830</td>\n",
       "      <td>14</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>1454</td>\n",
       "      <td>1</td>\n",
       "      <td>420</td>\n",
       "      <td>THEFT FROM MOTOR VEHICLE - PETTY ($950 &amp; UNDER)</td>\n",
       "      <td>...</td>\n",
       "      <td>IC</td>\n",
       "      <td>Invest Cont</td>\n",
       "      <td>420.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4700    LA VILLA MARINA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.9813</td>\n",
       "      <td>-118.4350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004986</th>\n",
       "      <td>252104112</td>\n",
       "      <td>02/02/2025 12:00:00 AM</td>\n",
       "      <td>02/02/2025 12:00:00 AM</td>\n",
       "      <td>130</td>\n",
       "      <td>21</td>\n",
       "      <td>Topanga</td>\n",
       "      <td>2103</td>\n",
       "      <td>2</td>\n",
       "      <td>946</td>\n",
       "      <td>OTHER MISCELLANEOUS CRIME</td>\n",
       "      <td>...</td>\n",
       "      <td>IC</td>\n",
       "      <td>Invest Cont</td>\n",
       "      <td>946.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22100    ROSCOE                       BL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.2259</td>\n",
       "      <td>-118.6126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004987</th>\n",
       "      <td>250404100</td>\n",
       "      <td>02/18/2025 12:00:00 AM</td>\n",
       "      <td>02/18/2025 12:00:00 AM</td>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "      <td>Hollenbeck</td>\n",
       "      <td>479</td>\n",
       "      <td>2</td>\n",
       "      <td>237</td>\n",
       "      <td>CHILD NEGLECT (SEE 300 W.I.C.)</td>\n",
       "      <td>...</td>\n",
       "      <td>IC</td>\n",
       "      <td>Invest Cont</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3500    PERCY                        ST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.0277</td>\n",
       "      <td>-118.1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004988</th>\n",
       "      <td>251304095</td>\n",
       "      <td>01/31/2025 12:00:00 AM</td>\n",
       "      <td>01/30/2025 12:00:00 AM</td>\n",
       "      <td>1554</td>\n",
       "      <td>13</td>\n",
       "      <td>Newton</td>\n",
       "      <td>1372</td>\n",
       "      <td>2</td>\n",
       "      <td>850</td>\n",
       "      <td>INDECENT EXPOSURE</td>\n",
       "      <td>...</td>\n",
       "      <td>IC</td>\n",
       "      <td>Invest Cont</td>\n",
       "      <td>850.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300 E  53RD                         ST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.9942</td>\n",
       "      <td>-118.2701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004989</th>\n",
       "      <td>251704066</td>\n",
       "      <td>01/17/2025 12:00:00 AM</td>\n",
       "      <td>01/17/2025 12:00:00 AM</td>\n",
       "      <td>1600</td>\n",
       "      <td>17</td>\n",
       "      <td>Devonshire</td>\n",
       "      <td>1774</td>\n",
       "      <td>2</td>\n",
       "      <td>624</td>\n",
       "      <td>BATTERY - SIMPLE ASSAULT</td>\n",
       "      <td>...</td>\n",
       "      <td>IC</td>\n",
       "      <td>Invest Cont</td>\n",
       "      <td>624.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9600    ZELZAH                       AV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.2450</td>\n",
       "      <td>-118.5233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004990</th>\n",
       "      <td>251904210</td>\n",
       "      <td>03/25/2025 12:00:00 AM</td>\n",
       "      <td>03/25/2025 12:00:00 AM</td>\n",
       "      <td>1235</td>\n",
       "      <td>19</td>\n",
       "      <td>Mission</td>\n",
       "      <td>1944</td>\n",
       "      <td>2</td>\n",
       "      <td>850</td>\n",
       "      <td>INDECENT EXPOSURE</td>\n",
       "      <td>...</td>\n",
       "      <td>IC</td>\n",
       "      <td>Invest Cont</td>\n",
       "      <td>850.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11100    OMELVENY                     AV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.2722</td>\n",
       "      <td>-118.4417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1004991 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             DR_NO               Date Rptd                DATE OCC  TIME OCC  \\\n",
       "0        211507896  04/11/2021 12:00:00 AM  11/07/2020 12:00:00 AM       845   \n",
       "1        201516622  10/21/2020 12:00:00 AM  10/18/2020 12:00:00 AM      1845   \n",
       "2        240913563  12/10/2024 12:00:00 AM  10/30/2020 12:00:00 AM      1240   \n",
       "3        210704711  12/24/2020 12:00:00 AM  12/24/2020 12:00:00 AM      1310   \n",
       "4        201418201  10/03/2020 12:00:00 AM  09/29/2020 12:00:00 AM      1830   \n",
       "...            ...                     ...                     ...       ...   \n",
       "1004986  252104112  02/02/2025 12:00:00 AM  02/02/2025 12:00:00 AM       130   \n",
       "1004987  250404100  02/18/2025 12:00:00 AM  02/18/2025 12:00:00 AM      1000   \n",
       "1004988  251304095  01/31/2025 12:00:00 AM  01/30/2025 12:00:00 AM      1554   \n",
       "1004989  251704066  01/17/2025 12:00:00 AM  01/17/2025 12:00:00 AM      1600   \n",
       "1004990  251904210  03/25/2025 12:00:00 AM  03/25/2025 12:00:00 AM      1235   \n",
       "\n",
       "         AREA    AREA NAME  Rpt Dist No  Part 1-2  Crm Cd  \\\n",
       "0          15  N Hollywood         1502         2     354   \n",
       "1          15  N Hollywood         1521         1     230   \n",
       "2           9     Van Nuys          933         2     354   \n",
       "3           7     Wilshire          782         1     331   \n",
       "4          14      Pacific         1454         1     420   \n",
       "...       ...          ...          ...       ...     ...   \n",
       "1004986    21      Topanga         2103         2     946   \n",
       "1004987     4   Hollenbeck          479         2     237   \n",
       "1004988    13       Newton         1372         2     850   \n",
       "1004989    17   Devonshire         1774         2     624   \n",
       "1004990    19      Mission         1944         2     850   \n",
       "\n",
       "                                               Crm Cd Desc  ... Status  \\\n",
       "0                                        THEFT OF IDENTITY  ...     IC   \n",
       "1           ASSAULT WITH DEADLY WEAPON, AGGRAVATED ASSAULT  ...     IC   \n",
       "2                                        THEFT OF IDENTITY  ...     IC   \n",
       "3        THEFT FROM MOTOR VEHICLE - GRAND ($950.01 AND ...  ...     IC   \n",
       "4          THEFT FROM MOTOR VEHICLE - PETTY ($950 & UNDER)  ...     IC   \n",
       "...                                                    ...  ...    ...   \n",
       "1004986                          OTHER MISCELLANEOUS CRIME  ...     IC   \n",
       "1004987                     CHILD NEGLECT (SEE 300 W.I.C.)  ...     IC   \n",
       "1004988                                  INDECENT EXPOSURE  ...     IC   \n",
       "1004989                           BATTERY - SIMPLE ASSAULT  ...     IC   \n",
       "1004990                                  INDECENT EXPOSURE  ...     IC   \n",
       "\n",
       "         Status Desc Crm Cd 1 Crm Cd 2  Crm Cd 3 Crm Cd 4  \\\n",
       "0        Invest Cont    354.0      NaN       NaN      NaN   \n",
       "1        Invest Cont    230.0      NaN       NaN      NaN   \n",
       "2        Invest Cont    354.0      NaN       NaN      NaN   \n",
       "3        Invest Cont    331.0      NaN       NaN      NaN   \n",
       "4        Invest Cont    420.0      NaN       NaN      NaN   \n",
       "...              ...      ...      ...       ...      ...   \n",
       "1004986  Invest Cont    946.0      NaN       NaN      NaN   \n",
       "1004987  Invest Cont    237.0      NaN       NaN      NaN   \n",
       "1004988  Invest Cont    850.0      NaN       NaN      NaN   \n",
       "1004989  Invest Cont    624.0      NaN       NaN      NaN   \n",
       "1004990  Invest Cont    850.0      NaN       NaN      NaN   \n",
       "\n",
       "                                         LOCATION Cross Street      LAT  \\\n",
       "0         7800    BEEMAN                       AV          NaN  34.2124   \n",
       "1                 ATOLL                        AV     N  GAULT  34.1993   \n",
       "2        14600    SYLVAN                       ST          NaN  34.1847   \n",
       "3         6000    COMEY                        AV          NaN  34.0339   \n",
       "4                         4700    LA VILLA MARINA          NaN  33.9813   \n",
       "...                                           ...          ...      ...   \n",
       "1004986  22100    ROSCOE                       BL          NaN  34.2259   \n",
       "1004987   3500    PERCY                        ST          NaN  34.0277   \n",
       "1004988    300 E  53RD                         ST          NaN  33.9942   \n",
       "1004989   9600    ZELZAH                       AV          NaN  34.2450   \n",
       "1004990  11100    OMELVENY                     AV          NaN  34.2722   \n",
       "\n",
       "              LON  \n",
       "0       -118.4092  \n",
       "1       -118.4203  \n",
       "2       -118.4509  \n",
       "3       -118.3747  \n",
       "4       -118.4350  \n",
       "...           ...  \n",
       "1004986 -118.6126  \n",
       "1004987 -118.1979  \n",
       "1004988 -118.2701  \n",
       "1004989 -118.5233  \n",
       "1004990 -118.4417  \n",
       "\n",
       "[1004991 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Switch to your own path\n",
    "df = pd.read_csv(\"Crime_Data_from_2020_to_Present.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\n",
    "    'Status', 'Status Desc', 'Crm Cd', 'Crm Cd 1', 'Crm Cd 2', \n",
    "    'Crm Cd 3', 'Crm Cd 4', 'DR_NO', 'LOCATION', 'Cross Street'\n",
    "], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kotha\\AppData\\Local\\Temp\\ipykernel_26884\\3210269932.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\kotha\\AppData\\Local\\Temp\\ipykernel_26884\\3210269932.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "for col in ['Date Rptd', 'DATE OCC']:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['Crm Cd Desc', 'Vict Age', 'Vict Sex', 'Vict Descent', 'LAT', 'LON'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['Vict Age'] > 0) & (df['Vict Age'] < 120)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_crime_type(desc):\n",
    "    desc = str(desc).lower()\n",
    "\n",
    "    # --- Theft / financial crimes ---\n",
    "    if any(word in desc for word in [\n",
    "        \"theft\", \"robbery\", \"burglary\", \"stolen\", \"shoplift\", \"stealing\",\n",
    "        \"pickpocket\", \"purse snatching\", \"till tap\", \"forgery\", \"embezzlement\",\n",
    "        \"bunco\", \"prowler\", \"larceny\", \"credit card\", \"fraud\", \"counterfeit\",\n",
    "        \"document worthless\", \"stolen property\"\n",
    "    ]):\n",
    "        return \"Theft\"\n",
    "\n",
    "    # --- Assault / violent interpersonal crimes ---\n",
    "    elif any(word in desc for word in [\"assault\", \"battery\", \"fighting\", \"mayhem\"]):\n",
    "        return \"Assault\"\n",
    "\n",
    "    # --- Homicide-related ---\n",
    "    elif any(word in desc for word in [\"homicide\", \"murder\", \"manslaughter\"]):\n",
    "        return \"Homicide\"\n",
    "\n",
    "    # --- Sexual crimes ---\n",
    "    elif any(word in desc for word in [\n",
    "        \"rape\", \"sexual\", \"lewd\", \"oral copulation\", \"indecent\", \"molest\",\n",
    "        \"child pornography\", \"sex\", \"peeping tom\", \"pimping\"\n",
    "    ]):\n",
    "        return \"Sexual Crime\"\n",
    "\n",
    "    # --- Property damage / vandalism / trespass ---\n",
    "    elif any(word in desc for word in [\n",
    "        \"vandalism\", \"arson\", \"trespass\", \"malicious mischief\", \"graffiti\",\n",
    "        \"illegal dumping\", \"property damage\", \"telephone\"\n",
    "    ]):\n",
    "        return \"Property Crime\"\n",
    "\n",
    "    # --- Drugs / alcohol ---\n",
    "    elif any(word in desc for word in [\"drunk\", \"narcotic\", \"drug\", \"alcohol\", \"under influence\"]):\n",
    "        return \"Drug/Alcohol\"\n",
    "\n",
    "    # --- Weapons / shootings ---\n",
    "    elif any(word in desc for word in [\"weapon\", \"firearm\", \"gun\", \"shooting\", \"shots fired\"]):\n",
    "        return \"Weapon Offense\"\n",
    "\n",
    "    # --- Kidnapping or abduction ---\n",
    "    elif any(word in desc for word in [\"kidnap\", \"abduction\", \"child stealing\", \"false imprisonment\"]):\n",
    "        return \"Kidnapping\"\n",
    "\n",
    "    # --- Threats, harassment, stalking, restraining order, or disruption ---\n",
    "    elif any(word in desc for word in [\n",
    "        \"threat\", \"extortion\", \"stalking\", \"harass\", \"intimidation\", \"restraining order\",\n",
    "        \"court order\", \"contempt\", \"violation of restraining\", \"disturbing the peace\",\n",
    "        \"disrupt school\", \"riot\", \"lynching\", \"bomb\"\n",
    "    ]):\n",
    "        return \"Threat/Intimidation\"\n",
    "\n",
    "    # --- Vehicle / traffic crimes ---\n",
    "    elif any(word in desc for word in [\"vehicle\", \"traffic\", \"hit and run\", \"failure to yield\", \"driving\"]):\n",
    "        return \"Vehicle Crime\"\n",
    "\n",
    "    # --- Family or child-related crimes ---\n",
    "    elif any(word in desc for word in [\n",
    "        \"child\", \"chld\", \"abandonment\", \"neglect\", \"runaway\", \"custody\", \"domestic\",\n",
    "        \"pandering\", \"contributing\", \"cruelty to animals\", \"family\", \"disrupt school\", \"bigamy\"\n",
    "    ]):\n",
    "        return \"Family/Child Issue\"\n",
    "\n",
    "    # --- Cyber / computer crimes ---\n",
    "    elif any(word in desc for word in [\"computer\", \"cyber\", \"unauthorized access\", \"hacking\"]):\n",
    "        return \"Cyber Crime\"\n",
    "\n",
    "    # --- Miscellaneous or rare ---\n",
    "    else:\n",
    "        return \"Other\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime_Category\n",
      "Theft                  367298\n",
      "Assault                197608\n",
      "Property Crime          76724\n",
      "Weapon Offense          35269\n",
      "Threat/Intimidation     25905\n",
      "Sexual Crime            19388\n",
      "Other                    4713\n",
      "Family/Child Issue       3985\n",
      "Homicide                 1552\n",
      "Vehicle Crime            1387\n",
      "Kidnapping               1260\n",
      "Cyber Crime               463\n",
      "Drug/Alcohol               44\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kotha\\AppData\\Local\\Temp\\ipykernel_26884\\1498166902.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Crime_Category\"] = df[\"Crm Cd Desc\"].apply(simplify_crime_type)\n"
     ]
    }
   ],
   "source": [
    "# Apply the simplification\n",
    "df[\"Crime_Category\"] = df[\"Crm Cd Desc\"].apply(simplify_crime_type)\n",
    "\n",
    "# Check distribution\n",
    "print(df[\"Crime_Category\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kotha\\AppData\\Local\\Temp\\ipykernel_26884\\3521649628.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Crime_Category\"] = df[\"Crm Cd Desc\"].apply(simplify_crime_type)\n"
     ]
    }
   ],
   "source": [
    "df[\"Crime_Category\"] = df[\"Crm Cd Desc\"].apply(simplify_crime_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kotha\\AppData\\Local\\Temp\\ipykernel_26884\\3548523765.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"OCC_Year\"] = df[\"DATE OCC\"].dt.year\n",
      "C:\\Users\\kotha\\AppData\\Local\\Temp\\ipykernel_26884\\3548523765.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"OCC_Month\"] = df[\"DATE OCC\"].dt.month\n",
      "C:\\Users\\kotha\\AppData\\Local\\Temp\\ipykernel_26884\\3548523765.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"OCC_Day\"] = df[\"DATE OCC\"].dt.day\n",
      "C:\\Users\\kotha\\AppData\\Local\\Temp\\ipykernel_26884\\3548523765.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"OCC_Weekday\"] = df[\"DATE OCC\"].dt.day_name()\n",
      "C:\\Users\\kotha\\AppData\\Local\\Temp\\ipykernel_26884\\3548523765.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"OCC_Hour\"] = df[\"TIME OCC\"] // 100\n"
     ]
    }
   ],
   "source": [
    "df[\"OCC_Year\"] = df[\"DATE OCC\"].dt.year\n",
    "df[\"OCC_Month\"] = df[\"DATE OCC\"].dt.month\n",
    "df[\"OCC_Day\"] = df[\"DATE OCC\"].dt.day\n",
    "df[\"OCC_Weekday\"] = df[\"DATE OCC\"].dt.day_name()\n",
    "df[\"OCC_Hour\"] = df[\"TIME OCC\"] // 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_cols = [\"Vict Age\", \"Vict Sex\", \"Vict Descent\"]\n",
    "contextual_cols = [\"AREA\", \"AREA NAME\", \"Rpt Dist No\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cols = demo_cols + contextual_cols + [\n",
    "    \"Part 1-2\", \"OCC_Year\", \"OCC_Month\", \"OCC_Weekday\", \"OCC_Hour\"\n",
    "]\n",
    "\n",
    "df_model = df[model_cols + [\"Crime_Category\"]].dropna().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (735596, 12)\n",
      "\n",
      "Columns ready for modeling:\n",
      " ['Vict Age', 'Vict Sex', 'Vict Descent', 'AREA', 'AREA NAME', 'Rpt Dist No', 'Part 1-2', 'OCC_Year', 'OCC_Month', 'OCC_Weekday', 'OCC_Hour', 'Crime_Category']\n",
      "\n",
      "Target distribution:\n",
      " Crime_Category\n",
      "Theft                  367298\n",
      "Assault                197608\n",
      "Property Crime          76724\n",
      "Weapon Offense          35269\n",
      "Threat/Intimidation     25905\n",
      "Sexual Crime            19388\n",
      "Other                    4713\n",
      "Family/Child Issue       3985\n",
      "Homicide                 1552\n",
      "Vehicle Crime            1387\n",
      "Kidnapping               1260\n",
      "Cyber Crime               463\n",
      "Drug/Alcohol               44\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape:\", df_model.shape)\n",
    "print(\"\\nColumns ready for modeling:\\n\", df_model.columns.tolist())\n",
    "print(\"\\nTarget distribution:\\n\", df_model[\"Crime_Category\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_model.drop(columns=[\"Crime_Category\"])\n",
    "y = df_model[\"Crime_Category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "num_cols = [c for c in X.columns if np.issubdtype(X[c].dtype, np.number)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_pipe = Pipeline([\n",
    "    (\"pre\", preprocessor),\n",
    "    (\"model\", LogisticRegression(max_iter=300, n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_pipe.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "            Assault       0.42      0.57      0.49     39522\n",
      "        Cyber Crime       0.00      0.00      0.00        92\n",
      "       Drug/Alcohol       0.00      0.00      0.00         9\n",
      " Family/Child Issue       0.49      0.36      0.41       797\n",
      "           Homicide       0.00      0.00      0.00       310\n",
      "         Kidnapping       0.00      0.00      0.00       252\n",
      "              Other       0.00      0.00      0.00       943\n",
      "     Property Crime       0.38      0.12      0.19     15345\n",
      "       Sexual Crime       0.00      0.00      0.00      3878\n",
      "              Theft       0.71      0.85      0.77     73460\n",
      "Threat/Intimidation       0.00      0.00      0.00      5181\n",
      "      Vehicle Crime       0.00      0.00      0.00       277\n",
      "     Weapon Offense       0.00      0.00      0.00      7054\n",
      "\n",
      "           accuracy                           0.59    147120\n",
      "          macro avg       0.15      0.15      0.14    147120\n",
      "       weighted avg       0.51      0.59      0.54    147120\n",
      "\n",
      "Accuracy: 0.5938\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Results:\")\n",
    "print(classification_report(y_test, y_pred_logreg, zero_division=0))\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred_logreg), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2: Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_pipe = Pipeline([\n",
    "    (\"pre\", preprocessor),\n",
    "    (\"model\", GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgb_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m y_pred_gb \u001b[38;5;241m=\u001b[39m gb_pipe\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\kotha\\Downloads\\DTSC-3602-Group-12-Project\\.venv\\lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kotha\\Downloads\\DTSC-3602-Group-12-Project\\.venv\\lib\\site-packages\\sklearn\\pipeline.py:663\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    658\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    659\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    660\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    661\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    662\u001b[0m         )\n\u001b[1;32m--> 663\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kotha\\Downloads\\DTSC-3602-Group-12-Project\\.venv\\lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kotha\\Downloads\\DTSC-3602-Group-12-Project\\.venv\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:787\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\kotha\\Downloads\\DTSC-3602-Group-12-Project\\.venv\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:883\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    876\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss(\n\u001b[0;32m    877\u001b[0m             y_true\u001b[38;5;241m=\u001b[39my_oob_masked,\n\u001b[0;32m    878\u001b[0m             raw_prediction\u001b[38;5;241m=\u001b[39mraw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    879\u001b[0m             sample_weight\u001b[38;5;241m=\u001b[39msample_weight_oob_masked,\n\u001b[0;32m    880\u001b[0m         )\n\u001b[0;32m    882\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 883\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32mc:\\Users\\kotha\\Downloads\\DTSC-3602-Group-12-Project\\.venv\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:489\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    486\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    488\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csc \u001b[38;5;28;01mif\u001b[39;00m X_csc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 489\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_g_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    494\u001b[0m X_for_tree_update \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n",
      "File \u001b[1;32mc:\\Users\\kotha\\Downloads\\DTSC-3602-Group-12-Project\\.venv\\lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kotha\\Downloads\\DTSC-3602-Group-12-Project\\.venv\\lib\\site-packages\\sklearn\\tree\\_classes.py:1404\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m \n\u001b[0;32m   1378\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1404\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kotha\\Downloads\\DTSC-3602-Group-12-Project\\.venv\\lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gb_pipe.fit(X_train, y_train)\n",
    "y_pred_gb = gb_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gradient Boosting Results\")\n",
    "print(classification_report(y_test, y_pred_gb, zero_division=0))\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred_gb), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe = Pipeline([\n",
    "    (\"pre\", preprocessor),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "        n_estimators=150, max_depth=15, n_jobs=-1, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe.fit(X_train, y_train)\n",
    "y_pred_rf = rf_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest Results\")\n",
    "print(classification_report(y_test, y_pred_rf, zero_division=0))\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred_rf), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fairness Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map LAPD Vict Descent codes to full subgroup names (make reading fairness easier)\n",
    "descent_mapping = {\n",
    "    \"A\": \"Other Asian\",\n",
    "    \"B\": \"Black\",\n",
    "    \"C\": \"Chinese\",\n",
    "    \"D\": \"Cambodian\",\n",
    "    \"F\": \"Filipino\",\n",
    "    \"G\": \"Guamanian\",\n",
    "    \"H\": \"Hispanic/Latino\",\n",
    "    \"I\": \"American Indian/Alaskan Native\",\n",
    "    \"J\": \"Japanese\",\n",
    "    \"K\": \"Korean\",\n",
    "    \"L\": \"Laotian\",\n",
    "    \"O\": \"Other\",\n",
    "    \"P\": \"Pacific Islander\",\n",
    "    \"S\": \"Samoan\",\n",
    "    \"U\": \"Hawaiian\",\n",
    "    \"V\": \"Vietnamese\",\n",
    "    \"W\": \"White\",\n",
    "    \"X\": \"Unknown\",\n",
    "    \"Z\": \"Asian Indian\"\n",
    "}\n",
    "\n",
    "df[\"Vict Descent Full\"] = df[\"Vict Descent\"].map(descent_mapping).fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Fairness\n",
    "df_logisticreg = pd.DataFrame({\n",
    "    \"y_true\" : y_test,\n",
    "    \"y_pred\" : y_pred_logreg,\n",
    "    \"group\" :  df.loc[y_test.index, \"Vict Descent Full\"]\n",
    "})\n",
    "\n",
    "acc_by_group = df_logisticreg.groupby(\"group\").apply(lambda x: accuracy_score(x.y_true, x.y_pred))\n",
    "gap = acc_by_group.max() - acc_by_group.min()\n",
    "\n",
    "print(\"Fairness Results â€” Logistic Regression\")\n",
    "print(acc_by_group.sort_values(ascending=False).round(3))\n",
    "print(f\"\\nSubgroup Accuracy Gap: {gap:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boost Fairness\n",
    "df_gb = pd.DataFrame({\n",
    "    \"y_true\": y_test,\n",
    "    \"y_pred\": y_pred_gb,\n",
    "    \"group\": df.loc[y_test.index, \"Vict Descent Full\"]\n",
    "})\n",
    "\n",
    "acc_by_group_gb = df_gb.groupby(\"group\").apply(lambda x: accuracy_score(x.y_true, x.y_pred))\n",
    "gap_gb = acc_by_group_gb.max() - acc_by_group_gb.min()\n",
    "\n",
    "print(\"Fairness Results â€” Gradient Boosting\")\n",
    "print(acc_by_group_gb.sort_values(ascending=False).round(3))\n",
    "print(f\"\\nSubgroup Accuracy Gap: {gap_gb:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Fairness\n",
    "df_rf = pd.DataFrame({\n",
    "    \"y_true\": y_test,\n",
    "    \"y_pred\": y_pred_rf,\n",
    "    \"group\": df.loc[y_test.index, \"Vict Descent Full\"]\n",
    "})\n",
    "\n",
    "acc_by_group_rf = df_rf.groupby(\"group\").apply(lambda x: accuracy_score(x.y_true, x.y_pred))\n",
    "gap_rf = acc_by_group_rf.max() - acc_by_group_rf.min()\n",
    "\n",
    "print(\"Fairness Results â€” Random Forest\")\n",
    "print(acc_by_group_rf.sort_values(ascending=False).round(3))\n",
    "print(f\"\\nSubgroup Accuracy Gap: {gap_rf:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall\n",
    "fairness_summary = pd.DataFrame({\n",
    "    \"Model\": [\"Logistic Regression\", \"Random Forest\", \"Gradient Boosting\"],\n",
    "    \"Subgroup Accuracy Gap\": [gap, gap_rf, gap_gb]\n",
    "}).sort_values(\"Subgroup Accuracy Gap\")\n",
    "\n",
    "print(\"\\nOverall Fairness Comparison:\")\n",
    "print(fairness_summary.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression \n",
    "log_model = logreg_pipe.named_steps[\"model\"]\n",
    "pre = logreg_pipe.named_steps[\"pre\"]\n",
    "\n",
    "coefs = log_model.coef_[0]  # one-vs-rest (first class)\n",
    "feature_names = pre.get_feature_names_out()\n",
    "\n",
    "idx = np.argsort(np.abs(coefs))[-10:]\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(np.array(feature_names)[idx], np.abs(coefs[idx]))\n",
    "plt.title(\"Logistic Regression â€” Top 15 Coefficients (by Magnitude)\")\n",
    "plt.xlabel(\"|Coefficient|\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boost \n",
    "gb_model = gb_pipe.named_steps[\"model\"]\n",
    "pre = gb_pipe.named_steps[\"pre\"]\n",
    "\n",
    "feature_names = pre.get_feature_names_out()\n",
    "importances = gb_model.feature_importances_\n",
    "\n",
    "idx = np.argsort(importances)[-10:]\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(np.array(feature_names)[idx], importances[idx])\n",
    "plt.title(\"Gradient Boosting â€” Top 15 Feature Importances\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = rf_pipe.named_steps[\"model\"]\n",
    "pre = rf_pipe.named_steps[\"pre\"]\n",
    "\n",
    "feature_names = pre.get_feature_names_out()\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Sort and select top 15\n",
    "idx = np.argsort(importances)[-10:]\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(np.array(feature_names)[idx], importances[idx])\n",
    "plt.title(\"Random Forest â€” Top 15 Feature Importances\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Analysis\n",
    "top_features_rf = pd.DataFrame({\n",
    "    \"Feature\": np.array(feature_names)[idx],\n",
    "    \"Importance\": importances[idx]\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "display(top_features_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Improved Representation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age bins (interpretable)\n",
    "df_imp[\"Vict Age Bin\"] = pd.cut(\n",
    "    df_imp[\"Vict Age\"],\n",
    "    bins=[0, 17, 29, 49, 64, 120],\n",
    "    labels=[\"0-17\",\"18-29\",\"30-49\",\"50-64\",\"65+\"],\n",
    "    right=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group rare AREA NAMEs\n",
    "min_freq = 0.01  \n",
    "area_counts = df_imp[\"AREA NAME\"].value_counts(normalize=True)\n",
    "rare_areas = area_counts[area_counts < min_freq].index\n",
    "df_imp[\"AREA NAME Grouped\"] = df_imp[\"AREA NAME\"].where(~df_imp[\"AREA NAME\"].isin(rare_areas), \"Other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same datetime features as baseline\n",
    "df_imp[\"OCC_Year\"] = df_imp[\"DATE OCC\"].dt.year\n",
    "df_imp[\"OCC_Month\"] = df_imp[\"DATE OCC\"].dt.month\n",
    "df_imp[\"OCC_Weekday\"] = df_imp[\"DATE OCC\"].dt.day_name()\n",
    "df_imp[\"OCC_Hour\"] = df_imp[\"TIME OCC\"] // 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_cols_imp = [\"Vict Age Bin\"]\n",
    "context_cols_imp = [\"AREA\", \"AREA NAME Grouped\", \"Rpt Dist No\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cols_imp = demo_cols_imp + context_cols_imp + [\"Part 1-2\",\"OCC_Year\",\"OCC_Month\",\"OCC_Weekday\",\"OCC_Hour\"]\n",
    "df_model_improved = df_imp[model_cols_imp + [\"Crime_Category\",\"Vict Descent Full\"]].dropna().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imp = df_model_improved.drop(columns=[\"Crime_Category\", \"Vict Descent Full\"])\n",
    "y_imp = df_model_improved[\"Crime_Category\"]\n",
    "sens_imp = df_model_improved[\"Vict Descent Full\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_imp, Xte_imp, ytr_imp, yte_imp = train_test_split(\n",
    "    X_imp, y_imp, test_size=0.2, stratify=y_imp, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols_imp = [\n",
    "    c for c in X_imp.columns \n",
    "    if pd.api.types.is_object_dtype(X_imp[c]) or pd.api.types.is_categorical_dtype(X_imp[c])\n",
    "]\n",
    "\n",
    "num_cols_imp = [\n",
    "    c for c in X_imp.columns \n",
    "    if pd.api.types.is_numeric_dtype(X_imp[c])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_imp = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_cols_imp),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols_imp)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_imp = Pipeline([\n",
    "    (\"pre\", pre_imp),\n",
    "    (\"model\", LogisticRegression(max_iter=300, n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_imp.fit(Xtr_imp, ytr_imp)\n",
    "y_pred_log_imp = logreg_imp.predict(Xte_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear Regression (Improved)\")\n",
    "print(classification_report(yte_imp, y_pred_log_imp, zero_division=0))\n",
    "print(\"Accuracy:\", round(accuracy_score(yte_imp, y_pred_log_imp), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2: Gradient Boost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_imp = Pipeline([\n",
    "    (\"pre\", pre_imp),\n",
    "    (\"model\", GradientBoostingClassifier(\n",
    "        n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_imp.fit(Xtr_imp, ytr_imp)\n",
    "y_pred_gb_imp = gb_imp.predict(Xte_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gradient Boost (Improved)\")\n",
    "print(classification_report(yte_imp, y_pred_gb_imp, zero_division=0))\n",
    "print(\"Accuracy:\", round(accuracy_score(yte_imp, y_pred_gb_imp), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3: Random Forest (Improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_imp = Pipeline([\n",
    "    (\"pre\", pre_imp),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "        n_estimators=150, max_depth=15, n_jobs=-1, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_imp.fit(Xtr_imp, ytr_imp)\n",
    "y_pred_rf_imp = rf_imp.predict(Xte_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest (Improved)\")\n",
    "print(classification_report(yte_imp, y_pred_rf_imp, zero_division=0))\n",
    "print(\"Accuracy:\", round(accuracy_score(yte_imp, y_pred_rf_imp), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fairness Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Fairness (Improved)\n",
    "df_logreg_imp = pd.DataFrame({\n",
    "    \"y_true\": yte_imp,\n",
    "    \"y_pred\": y_pred_log_imp,\n",
    "    \"group\": sens_imp.loc[yte_imp.index]\n",
    "})\n",
    "\n",
    "acc_by_group = df_logreg_imp.groupby(\"group\").apply(lambda x: accuracy_score(x.y_true, x.y_pred))\n",
    "gap_log_imp = acc_by_group.max() - acc_by_group.min()\n",
    "\n",
    "print(\"Fairness Results â€” Logistic Regression (Improved)\")\n",
    "print(acc_by_group.sort_values(ascending=False).round(3))\n",
    "print(f\"\\nSubgroup Accuracy Gap: {gap:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Fairness (Improved)\n",
    "df_gb_imp = pd.DataFrame({\n",
    "    \"y_true\": yte_imp,\n",
    "    \"y_pred\": y_pred_gb_imp,\n",
    "    \"group\": sens_imp.loc[yte_imp.index]\n",
    "})\n",
    "\n",
    "acc_by_group = df_gb_imp.groupby(\"group\").apply(lambda x: accuracy_score(x.y_true, x.y_pred))\n",
    "gap_gb_imp = acc_by_group.max() - acc_by_group.min()\n",
    "\n",
    "print(\"Fairness Results â€” Gradient Boosting (Improved)\")\n",
    "print(acc_by_group.sort_values(ascending=False).round(3))\n",
    "print(f\"\\nSubgroup Accuracy Gap: {gap:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Fairness (Improved)\n",
    "df_rf_imp = pd.DataFrame({\n",
    "    \"y_true\": yte_imp,\n",
    "    \"y_pred\": y_pred_rf_imp,\n",
    "    \"group\": sens_imp.loc[yte_imp.index]\n",
    "})\n",
    "\n",
    "acc_by_group = df_rf_imp.groupby(\"group\").apply(lambda x: accuracy_score(x.y_true, x.y_pred))\n",
    "gap_rf_imp = acc_by_group.max() - acc_by_group.min()\n",
    "\n",
    "print(\"Fairness Results â€” Random Forest (Improved)\")\n",
    "print(acc_by_group.sort_values(ascending=False).round(3))\n",
    "print(f\"\\nSubgroup Accuracy Gap: {gap:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Fairness Summary\n",
    "fairness_summary = pd.DataFrame({\n",
    "    \"Model\": [\"Logistic Regression (Improved)\", \"Random Forest (Improved)\", \"Gradient Boosting (Improved)\"],\n",
    "    \"Subgroup Accuracy Gap\": [gap_log_imp, gap_rf_imp, gap_gb_imp]\n",
    "}).sort_values(\"Subgroup Accuracy Gap\")\n",
    "\n",
    "print(\"\\nOverall Fairness Comparison:\")\n",
    "print(fairness_summary.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "log_model = logreg_imp.named_steps[\"model\"]\n",
    "pre = logreg_imp.named_steps[\"pre\"]\n",
    "\n",
    "feature_names = pre.get_feature_names_out()\n",
    "coefs = log_model.coef_[0]\n",
    "\n",
    "idx = np.argsort(np.abs(coefs))[-10:]\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(np.array(feature_names)[idx], np.abs(coefs[idx]))\n",
    "plt.title(\"Logistic Regression (Improved) â€” Top 15 Coefficients by Magnitude\")\n",
    "plt.xlabel(\"|Coefficient|\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "gb_model = gb_imp.named_steps[\"model\"]\n",
    "pre = gb_imp.named_steps[\"pre\"]\n",
    "\n",
    "feature_names = pre.get_feature_names_out()\n",
    "importances = gb_model.feature_importances_\n",
    "\n",
    "idx = np.argsort(importances)[-10:]\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(np.array(feature_names)[idx], importances[idx])\n",
    "plt.title(\"Gradient Boosting (Improved) â€” Top 15 Feature Importances\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = rf_imp.named_steps[\"model\"]\n",
    "pre = rf_imp.named_steps[\"pre\"]\n",
    "\n",
    "feature_names = pre.get_feature_names_out()\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "idx = np.argsort(importances)[-10:]\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(np.array(feature_names)[idx], importances[idx])\n",
    "plt.title(\"Random Forest (Improved) â€” Top 15 Feature Importances\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comparing Baseline to Improved*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(yte_imp, y_pred_log_imp, output_dict=True, zero_division=0)\n",
    "f1_log_imp = report[\"weighted avg\"][\"f1-score\"]\n",
    "acc_log_imp = report[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Report from Baseline\n",
    "\n",
    "# Logistic Regression (Baseline)\n",
    "report_log = classification_report(y_test, y_pred_logreg, output_dict=True, zero_division=0)\n",
    "acc_log = report_log[\"accuracy\"]\n",
    "f1_log = report_log[\"weighted avg\"][\"f1-score\"]\n",
    "\n",
    "# Random Forest (Baseline)\n",
    "report_rf = classification_report(y_test, y_pred_rf, output_dict=True, zero_division=0)\n",
    "acc_rf = report_rf[\"accuracy\"]\n",
    "f1_rf = report_rf[\"weighted avg\"][\"f1-score\"]\n",
    "\n",
    "# Gradient Boosting (Baseline)\n",
    "report_gb = classification_report(y_test, y_pred_gb, output_dict=True, zero_division=0)\n",
    "acc_gb = report_gb[\"accuracy\"]\n",
    "f1_gb = report_gb[\"weighted avg\"][\"f1-score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Report from Improved\n",
    "\n",
    "# Logistic Regression (Improved)\n",
    "report_log_imp = classification_report(yte_imp, y_pred_log_imp, output_dict=True, zero_division=0)\n",
    "acc_log_imp = report_log_imp[\"accuracy\"]\n",
    "f1_log_imp = report_log_imp[\"weighted avg\"][\"f1-score\"]\n",
    "\n",
    "# Random Forest (Improved)\n",
    "report_rf_imp = classification_report(yte_imp, y_pred_rf_imp, output_dict=True, zero_division=0)\n",
    "acc_rf_imp = report_rf_imp[\"accuracy\"]\n",
    "f1_rf_imp = report_rf_imp[\"weighted avg\"][\"f1-score\"]\n",
    "\n",
    "# Gradient Boosting (Improved)\n",
    "report_gb_imp = classification_report(yte_imp, y_pred_gb_imp, output_dict=True, zero_division=0)\n",
    "acc_gb_imp = report_gb_imp[\"accuracy\"]\n",
    "f1_gb_imp = report_gb_imp[\"weighted avg\"][\"f1-score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logisticreg = pd.DataFrame({\n",
    "    \"y_true\" : y_test,\n",
    "    \"y_pred\" : y_pred_logreg,\n",
    "    \"group\" :  df.loc[y_test.index, \"Vict Descent Full\"]\n",
    "})\n",
    "acc_by_group = df_logisticreg.groupby(\"group\").apply(lambda x: accuracy_score(x.y_true, x.y_pred))\n",
    "gap = acc_by_group.max() - acc_by_group.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_log = gap   # after Logistic Regression baseline fairness\n",
    "gap_rf = gap    # after Random Forest baseline fairness\n",
    "gap_gb = gap    # after Gradient Boosting baseline fairness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    \"Model\": [\n",
    "        \"Logistic Regression (Baseline)\", \"Logistic Regression (Improved)\",\n",
    "        \"Random Forest (Baseline)\", \"Random Forest (Improved)\",\n",
    "        \"Gradient Boosting (Baseline)\", \"Gradient Boosting (Improved)\"\n",
    "    ],\n",
    "    \"Accuracy\": [\n",
    "        acc_log, acc_log_imp,\n",
    "        acc_rf, acc_rf_imp,\n",
    "        acc_gb, acc_gb_imp\n",
    "    ],\n",
    "    \"F1-Score\": [\n",
    "        f1_log, f1_log_imp,\n",
    "        f1_rf, f1_rf_imp,\n",
    "        f1_gb, f1_gb_imp\n",
    "    ],\n",
    "    \"Fairness Gap\": [\n",
    "        gap_log, gap_log_imp,\n",
    "        gap_rf, gap_rf_imp,\n",
    "        gap_gb, gap_gb_imp\n",
    "    ]\n",
    "})\n",
    "comparison_df.round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline vs Improved Visualizations\n",
    "\n",
    "models = [\"LogReg\", \"RandomForest\", \"GradBoost\"]\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "acc_baseline = [0.594, 0.613, 0.616]\n",
    "acc_improved = [0.585, 0.608, 0.607]\n",
    "\n",
    "f1_baseline = [0.539, 0.548, 0.565]\n",
    "f1_improved = [0.523, 0.541, 0.548]\n",
    "\n",
    "gap_baseline = [0.390, 0.390, 0.390]\n",
    "gap_improved = [0.383, 0.425, 0.362]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
    "titles = [\"Accuracy\", \"F1-Score\", \"Fairness Gap (Lower = Fairer)\"]\n",
    "metrics = [(acc_baseline, acc_improved),\n",
    "           (f1_baseline, f1_improved),\n",
    "           (gap_baseline, gap_improved)]\n",
    "\n",
    "for ax, (baseline, improved), title in zip(axes, metrics, titles):\n",
    "    ax.bar(x - width/2, baseline, width, label=\"Baseline\", alpha=0.8)\n",
    "    ax.bar(x + width/2, improved, width, label=\"Improved\", alpha=0.8)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.suptitle(\"Baseline vs Improved â€” Model Performance and Fairness\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
